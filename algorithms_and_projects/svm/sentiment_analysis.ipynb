{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5MB 6.8MB/s eta 0:00:01��             | 829kB 6.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (from nltk) (1.13.0)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jupyterlab/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.4.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jupyterlab/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "word_tokens = word_tokenize(example_sent)\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "ps = PorterStemmer()\n",
    "words =[\"program\", \"programs\", \"programer\", \"programing\", \"programers\"] \n",
    "stemmed_words = [ps.stem(w) for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "ps = PorterStemmer()\n",
    "words =[\"program\", \"programs\", \"programer\", \"programing\", \"programers\"] \n",
    "stemmed_words = [ps.stem(w) for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "ps = PorterStemmer()\n",
    "sentence = \"Programers program with programing languages\"\n",
    "tokenize_words = word_tokenize(sentence)\n",
    "stemmed_words = [ps.stem(w) for w in tokenize_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tk = TweetTokenizer()\n",
    "word = 'hello i am siddharth'\n",
    "tokenized_tweet = tk.tokenize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tk = TweetTokenizer()\n",
    "words = \":-) <> () {} [] :-p\"\n",
    "tokenized_tweets = tk.tokenize(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "     'This is the first document.',\n",
    "     'This document is the second document.',\n",
    "     'And this is the third one.',\n",
    "     'Is this the first document?',\n",
    " ]\n",
    "vectorize = CountVectorizer()\n",
    "X = vectorize.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
       "       [0, 2, 0, 1, 0, 1, 1, 0, 1],\n",
       "       [1, 0, 0, 1, 1, 0, 1, 1, 1],\n",
       "       [0, 1, 1, 1, 0, 0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=[\"One Cent, Two Cents, Old Cent, New Cent: All About Money\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cat_in_the_hat_docs = [\"One Cent, Two Cents, Old Cent, New Cent: All About Money (Cat in the Hat's Learning Library\",\n",
    "       \"Inside Your Outside: All About the Human Body (Cat in the Hat's Learning Library)\",\n",
    "       \"Oh, The Things You Can Do That Are Good for You: All About Staying Healthy (Cat in the Hat's Learning Library)\",\n",
    "       \"On Beyond Bugs: All About Insects (Cat in the Hat's Learning Library)\",\n",
    "       \"There's No Place Like Space: All About Our Solar System (Cat in the Hat's Learning Library)\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'one': 28,\n",
       " 'cent': 8,\n",
       " 'two': 40,\n",
       " 'cents': 9,\n",
       " 'old': 26,\n",
       " 'new': 23,\n",
       " 'all': 1,\n",
       " 'about': 0,\n",
       " 'money': 22,\n",
       " 'cat': 7,\n",
       " 'in': 16,\n",
       " 'the': 37,\n",
       " 'hat': 13,\n",
       " 'learning': 19,\n",
       " 'library': 20,\n",
       " 'inside': 18,\n",
       " 'your': 42,\n",
       " 'outside': 30,\n",
       " 'human': 15,\n",
       " 'body': 4,\n",
       " 'oh': 25,\n",
       " 'things': 39,\n",
       " 'you': 41,\n",
       " 'can': 6,\n",
       " 'do': 10,\n",
       " 'that': 36,\n",
       " 'are': 2,\n",
       " 'good': 12,\n",
       " 'for': 11,\n",
       " 'staying': 34,\n",
       " 'healthy': 14,\n",
       " 'on': 27,\n",
       " 'beyond': 3,\n",
       " 'bugs': 5,\n",
       " 'insects': 17,\n",
       " 'there': 38,\n",
       " 'no': 24,\n",
       " 'place': 31,\n",
       " 'like': 21,\n",
       " 'space': 33,\n",
       " 'our': 29,\n",
       " 'solar': 32,\n",
       " 'system': 35}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "cv.fit_transform(cat_in_the_hat_docs)\n",
    "cv.vocabulary_\n",
    "#get unique words\n",
    "#as we are using defaults this are word level tokens  loercaser the numbers are the position in the sparce vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 43)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x40 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 60 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing stopwords using custom corpora\n",
    "cv = CountVectorizer(stop_words = [\"all\",\"in\",\"the\",\"is\",\"and\"])\n",
    "cv.fit_transform(cat_in_the_hat_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all', 'in', 'the', 'is', 'and']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to check the stopwords being used\n",
    "cv.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.stop_words_ #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwords using min_df\n",
    "#the goal of min_df is to ignore words that have very few occurances to be considered meaningful\n",
    "#instead of using term_frequency(total_occurances of words) to eleminate words min_df looks at how many documents contain the term\n",
    "#hence it is known as document frequency min_df = 0.25 implies ignore words that have appeared 25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x8 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 40 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ignore words that have appeared in less than two documents or in this case array\n",
    "cv = CountVectorizer(min_df = 2)\n",
    "cv.fit_transform(cat_in_the_hat_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'are',\n",
       " 'beyond',\n",
       " 'body',\n",
       " 'bugs',\n",
       " 'can',\n",
       " 'cent',\n",
       " 'cents',\n",
       " 'do',\n",
       " 'for',\n",
       " 'good',\n",
       " 'healthy',\n",
       " 'human',\n",
       " 'insects',\n",
       " 'inside',\n",
       " 'like',\n",
       " 'money',\n",
       " 'new',\n",
       " 'no',\n",
       " 'oh',\n",
       " 'old',\n",
       " 'on',\n",
       " 'one',\n",
       " 'our',\n",
       " 'outside',\n",
       " 'place',\n",
       " 'solar',\n",
       " 'space',\n",
       " 'staying',\n",
       " 'system',\n",
       " 'that',\n",
       " 'there',\n",
       " 'things',\n",
       " 'two',\n",
       " 'you',\n",
       " 'your'}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to observe words eleminated\n",
    "cv.stop_words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'all': 1,\n",
       " 'about': 0,\n",
       " 'cat': 2,\n",
       " 'in': 4,\n",
       " 'the': 7,\n",
       " 'hat': 3,\n",
       " 'learning': 5,\n",
       " 'library': 6}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_ #to see whats remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop words using max df\n",
    "#just as we ignored words that were too rare using min_df we can ignore words that ate very common using max_df\n",
    "#if max_df is 0.85 ignore words that have appeared 85% of documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x35 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 35 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ignore terms that have appeared in 50% of the documents\n",
    "cv = CountVectorizer(max_df = 0.50)\n",
    "cv.fit_transform(cat_in_the_hat_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'about', 'all', 'cat', 'hat', 'in', 'learning', 'library', 'the'}"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.stop_words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'one': 21,\n",
       " 'cent': 5,\n",
       " 'two': 32,\n",
       " 'cents': 6,\n",
       " 'old': 19,\n",
       " 'new': 16,\n",
       " 'money': 15,\n",
       " 'inside': 13,\n",
       " 'your': 34,\n",
       " 'outside': 23,\n",
       " 'human': 11,\n",
       " 'body': 2,\n",
       " 'oh': 18,\n",
       " 'things': 31,\n",
       " 'you': 33,\n",
       " 'can': 4,\n",
       " 'do': 7,\n",
       " 'that': 29,\n",
       " 'are': 0,\n",
       " 'good': 9,\n",
       " 'for': 8,\n",
       " 'staying': 27,\n",
       " 'healthy': 10,\n",
       " 'on': 20,\n",
       " 'beyond': 1,\n",
       " 'bugs': 3,\n",
       " 'insects': 12,\n",
       " 'there': 30,\n",
       " 'no': 17,\n",
       " 'place': 24,\n",
       " 'like': 14,\n",
       " 'space': 26,\n",
       " 'our': 22,\n",
       " 'solar': 25,\n",
       " 'system': 28}"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x36 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 52 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#custom tokenization for keeping remove special characters and introduce a connector for certain words\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "#initializing the stemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "def my_cool_preprocessor(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\\\W',' ',text) #substituting space in place of special character\n",
    "    text = re.sub(\"\\\\s+(in|the|all|for|and|on)\\\\s+\",'_connector_',text) #normalize certain words\n",
    "    #stem words\n",
    "    words = re.split('\\\\s+',text)\n",
    "    stemmed_words = [porter_stemmer.stem(w) for w in words]\n",
    "    return ' '.join(stemmed_words)    \n",
    "cv = CountVectorizer(preprocessor = my_cool_preprocessor)  \n",
    "cv.fit_transform(cat_in_the_hat_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def my_tokenizer(text):\n",
    "    #create a space between special characters\n",
    "    text = re.sub('(\\\\W)','\\\\1',text) #if you see a special character keep the character intact\n",
    "    return re.split('\\\\s+',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doing', 'shit', '%$$$', 'dol$', 'specia@l', 'fuc@#er', 'MIL#$D']"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'doing shit %$$$ dol$ specia@l fuc@#er MIL#$D'\n",
    "my_tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x46 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 61 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extracting only bigrams\n",
    "cv = CountVectorizer(ngram_range = (2,2),preprocessor = my_cool_preprocessor)\n",
    "cv.fit_transform(cat_in_the_hat_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'one cent': 31,\n",
       " 'cent two': 20,\n",
       " 'two cent': 43,\n",
       " 'cent old': 19,\n",
       " 'old cent': 30,\n",
       " 'cent new': 18,\n",
       " 'new cent': 29,\n",
       " 'cent all': 17,\n",
       " 'all about': 9,\n",
       " 'about money': 5,\n",
       " 'money cat_connector_th': 28,\n",
       " 'cat_connector_th hat': 16,\n",
       " 'hat learn': 22,\n",
       " 'learn librari': 26,\n",
       " 'insid your': 25,\n",
       " 'your outsid': 45,\n",
       " 'outsid all': 33,\n",
       " 'all about_connector_human': 10,\n",
       " 'about_connector_human bodi': 8,\n",
       " 'bodi cat_connector_th': 13,\n",
       " 'Oh the': 2,\n",
       " 'the thing': 40,\n",
       " 'thing you': 42,\n",
       " 'you can': 44,\n",
       " 'can Do': 15,\n",
       " 'Do that': 0,\n",
       " 'that are': 39,\n",
       " 'are good_connector_y': 11,\n",
       " 'good_connector_y all': 21,\n",
       " 'about stay': 7,\n",
       " 'stay healthi': 37,\n",
       " 'healthi cat_connector_th': 23,\n",
       " 'On beyond': 3,\n",
       " 'beyond bug': 12,\n",
       " 'bug all': 14,\n",
       " 'about insect': 4,\n",
       " 'insect cat_connector_th': 24,\n",
       " 'there No': 41,\n",
       " 'No place': 1,\n",
       " 'place like': 34,\n",
       " 'like space': 27,\n",
       " 'space all': 36,\n",
       " 'about our': 6,\n",
       " 'our solar': 32,\n",
       " 'solar system': 35,\n",
       " 'system cat_connector_th': 38}"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x94 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 150 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#unigram and bigram\n",
    "cv = CountVectorizer(ngram_range = (1,2))\n",
    "cv.fit_transform(cat_in_the_hat_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'one': 61,\n",
       " 'cent': 20,\n",
       " 'two': 87,\n",
       " 'cents': 24,\n",
       " 'old': 57,\n",
       " 'new': 51,\n",
       " 'all': 6,\n",
       " 'about': 0,\n",
       " 'money': 49,\n",
       " 'cat': 18,\n",
       " 'in': 38,\n",
       " 'the': 79,\n",
       " 'hat': 32,\n",
       " 'learning': 44,\n",
       " 'library': 46,\n",
       " 'one cent': 62,\n",
       " 'cent two': 23,\n",
       " 'two cents': 88,\n",
       " 'cents old': 25,\n",
       " 'old cent': 58,\n",
       " 'cent new': 22,\n",
       " 'new cent': 52,\n",
       " 'cent all': 21,\n",
       " 'all about': 7,\n",
       " 'about money': 2,\n",
       " 'money cat': 50,\n",
       " 'cat in': 19,\n",
       " 'in the': 39,\n",
       " 'the hat': 80,\n",
       " 'hat learning': 33,\n",
       " 'learning library': 45,\n",
       " 'inside': 42,\n",
       " 'your': 92,\n",
       " 'outside': 65,\n",
       " 'human': 36,\n",
       " 'body': 12,\n",
       " 'inside your': 43,\n",
       " 'your outside': 93,\n",
       " 'outside all': 66,\n",
       " 'about the': 5,\n",
       " 'the human': 81,\n",
       " 'human body': 37,\n",
       " 'body cat': 13,\n",
       " 'oh': 55,\n",
       " 'things': 85,\n",
       " 'you': 89,\n",
       " 'can': 16,\n",
       " 'do': 26,\n",
       " 'that': 77,\n",
       " 'are': 8,\n",
       " 'good': 30,\n",
       " 'for': 28,\n",
       " 'staying': 73,\n",
       " 'healthy': 34,\n",
       " 'oh the': 56,\n",
       " 'the things': 82,\n",
       " 'things you': 86,\n",
       " 'you can': 91,\n",
       " 'can do': 17,\n",
       " 'do that': 27,\n",
       " 'that are': 78,\n",
       " 'are good': 9,\n",
       " 'good for': 31,\n",
       " 'for you': 29,\n",
       " 'you all': 90,\n",
       " 'about staying': 4,\n",
       " 'staying healthy': 74,\n",
       " 'healthy cat': 35,\n",
       " 'on': 59,\n",
       " 'beyond': 10,\n",
       " 'bugs': 14,\n",
       " 'insects': 40,\n",
       " 'on beyond': 60,\n",
       " 'beyond bugs': 11,\n",
       " 'bugs all': 15,\n",
       " 'about insects': 1,\n",
       " 'insects cat': 41,\n",
       " 'there': 83,\n",
       " 'no': 53,\n",
       " 'place': 67,\n",
       " 'like': 47,\n",
       " 'space': 71,\n",
       " 'our': 63,\n",
       " 'solar': 69,\n",
       " 'system': 75,\n",
       " 'there no': 84,\n",
       " 'no place': 54,\n",
       " 'place like': 68,\n",
       " 'like space': 48,\n",
       " 'space all': 72,\n",
       " 'about our': 3,\n",
       " 'our solar': 64,\n",
       " 'solar system': 70,\n",
       " 'system cat': 76}"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x10 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 50 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#character level bigrams only\n",
    "cv = CountVectorizer(ngram_range = (2,2),analyzer = 'char_wb',max_features = 10)\n",
    "cv.fit_transform(cat_in_the_hat_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'e ': 5,\n",
       " ' t': 2,\n",
       " ' a': 0,\n",
       " 'ou': 7,\n",
       " 't ': 8,\n",
       " 'at': 4,\n",
       " 'in': 6,\n",
       " 'th': 9,\n",
       " ' l': 1,\n",
       " 'ar': 3}"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x10 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 50 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#max feature returns maximum occurance of the words\n",
    "#only bigrams and unigrams and limit vocabulary size to 10\n",
    "cv = CountVectorizer(ngram_range = (1,2),max_features = 10)\n",
    "cv.fit_transform(cat_in_the_hat_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'about': 0,\n",
       " 'in': 4,\n",
       " 'the': 8,\n",
       " 'hat': 2,\n",
       " 'learning': 6,\n",
       " 'cat in': 1,\n",
       " 'in the': 5,\n",
       " 'the hat': 9,\n",
       " 'hat learning': 3,\n",
       " 'learning library': 7}"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
