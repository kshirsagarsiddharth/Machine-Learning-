{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing all the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import cmudict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "import syllables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the ciklist into Pandas dataframe and doing some preliminary analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(r'D:\\nlp_internship\\Data Science\\cik_list.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(152, 6)"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "    CIK            CONAME   FYRMO      FDATE     FORM  \\\n0  3662  SUNBEAM CORP/FL/  199803 1998-03-06  10-K405   \n1  3662  SUNBEAM CORP/FL/  199805 1998-05-15     10-Q   \n2  3662  SUNBEAM CORP/FL/  199808 1998-08-13  NT 10-Q   \n3  3662  SUNBEAM CORP/FL/  199811 1998-11-12   10-K/A   \n4  3662  SUNBEAM CORP/FL/  199811 1998-11-16  NT 10-Q   \n\n                                   SECFNAME  \n0  edgar/data/3662/0000950170-98-000413.txt  \n1  edgar/data/3662/0000950170-98-001001.txt  \n2  edgar/data/3662/0000950172-98-000783.txt  \n3  edgar/data/3662/0000950170-98-002145.txt  \n4  edgar/data/3662/0000950172-98-001203.txt  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CIK</th>\n      <th>CONAME</th>\n      <th>FYRMO</th>\n      <th>FDATE</th>\n      <th>FORM</th>\n      <th>SECFNAME</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3662</td>\n      <td>SUNBEAM CORP/FL/</td>\n      <td>199803</td>\n      <td>1998-03-06</td>\n      <td>10-K405</td>\n      <td>edgar/data/3662/0000950170-98-000413.txt</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3662</td>\n      <td>SUNBEAM CORP/FL/</td>\n      <td>199805</td>\n      <td>1998-05-15</td>\n      <td>10-Q</td>\n      <td>edgar/data/3662/0000950170-98-001001.txt</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3662</td>\n      <td>SUNBEAM CORP/FL/</td>\n      <td>199808</td>\n      <td>1998-08-13</td>\n      <td>NT 10-Q</td>\n      <td>edgar/data/3662/0000950172-98-000783.txt</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3662</td>\n      <td>SUNBEAM CORP/FL/</td>\n      <td>199811</td>\n      <td>1998-11-12</td>\n      <td>10-K/A</td>\n      <td>edgar/data/3662/0000950170-98-002145.txt</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3662</td>\n      <td>SUNBEAM CORP/FL/</td>\n      <td>199811</td>\n      <td>1998-11-16</td>\n      <td>NT 10-Q</td>\n      <td>edgar/data/3662/0000950172-98-001203.txt</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                CIK          FYRMO\ncount    152.000000     152.000000\nmean    5861.605263  200220.750000\nstd     2310.633436     429.991117\nmin     3662.000000  199402.000000\n25%     4447.000000  199906.000000\n50%     5907.000000  200101.500000\n75%     6201.000000  200605.500000\nmax    12239.000000  201407.000000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CIK</th>\n      <th>FYRMO</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>152.000000</td>\n      <td>152.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>5861.605263</td>\n      <td>200220.750000</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>2310.633436</td>\n      <td>429.991117</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>3662.000000</td>\n      <td>199402.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>4447.000000</td>\n      <td>199906.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>5907.000000</td>\n      <td>200101.500000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>6201.000000</td>\n      <td>200605.500000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>12239.000000</td>\n      <td>201407.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We need to extract the files from the web, hence hence lets add the hyperlink and extract all the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to extract the files from the web hence hence lets add the hyperlink and ectract all the text data\n",
    "hyperlink = 'https://www.sec.gov/Archives/'\n",
    "df['SECFNAME'] = hyperlink + df['SECFNAME']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing stopwords, positive words and negative words from the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = pd.read_csv(r'D:\\nlp_internship\\Data Science\\stop_words\\StopWords_GenericLong.txt',header=None) # importing stop words\n",
    "stopword_list = stop_words[0].to_list()\n",
    "master_dict = pd.read_csv(r'D:\\nlp_internship\\Data Science\\stop_words\\LoughranMcDonald_MasterDictionary_2018.csv') # importing the master dict\n",
    "negative_words = master_dict[master_dict['Negative'] == 2009]['Word']\n",
    "negative_dict =[word.lower() for word in negative_words if word not in stop_words] # creating negative words dictonary\n",
    "positive_words = master_dict[master_dict['Positive'] == 2009]['Word']\n",
    "positive_dict = [word.lower() for word in positive_words if word not in stop_words] # creating positive words dictonary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now all the dependencies are defined and all files are imported to proceed with the analysis. I saw that there are nearly 152 documents in the structure. Instead of downloading each file storing it in my SSD and loading it into my ram, and perform  analysis the text I decided to use, urllib library which allows me to load data from any webpage and store it in RAM. which is fast and efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preliminary_cleaner(text):\n",
    "    '''\n",
    "    This function is used to remove all the \n",
    "    numerical data in the files. And next line in the file\n",
    "    '''\n",
    "    if text:\n",
    "        array = []\n",
    "        soup =text\n",
    "        soup = re.sub('[\\d%/$]','',str(soup))\n",
    "        soup = re.sub(\"\\\\\\\\n\",'',soup)\n",
    "        soup = ' '.join(soup.split())\n",
    "        return soup\n",
    "        \n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now what I did is created a regular expression for all the three given titles. And then scraped the website for this data and stored in three python arrays. Later I merged the three arrays into a pandas data frame. Which is a highly efficient data-structure for processing the text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_one,array_two,array_three = [],[],[] # creating arrays\n",
    "# regular expression for the three components.\n",
    "mgt_disc_ana = r\"item[^a-zA-Z\\n]*\\d\\s*\\.\\s*management\\'s discussion and analysis.*?^\\s*item[^a-zA-Z\\n]*\\d\\s*\\.*\"\n",
    "qlty_qnt_disc = r\"item[^a-zA-Z\\n]*\\d[a-z]?\\.?\\s*Quantitative and Qualitative Disclosures about Market Risk.*?^\\s*item\\s*\\d\\s*\"\n",
    "rsk_fct = r\"item[^a-zA-Z\\n]*\\d[a-z]?\\.?\\s*Risk Factors.*?^\\s*item\\s*\\d\\s*\"\n",
    "for hyperlink in df['SECFNAME']:\n",
    "    text = urllib.request.urlopen(hyperlink).read().decode('utf-8') # eastiblishint the client server connection and downloading the data\n",
    "    # into ram.\n",
    "    matches_mda = re.findall(mgt_disc_ana, text, re.IGNORECASE | re.DOTALL | re.MULTILINE)\n",
    "    matches_qlty_qnt_disc= re.findall(qlty_qnt_disc,text,re.IGNORECASE | re.DOTALL | re.MULTILINE)\n",
    "    matches_rsk_fct = re.findall(rsk_fct,text,re.IGNORECASE | re.DOTALL | re.MULTILINE) \n",
    "    array_one.append(preliminary_cleaner(matches_mda))\n",
    "    array_two.append(preliminary_cleaner(matches_qlty_qnt_disc))\n",
    "    array_three.append(preliminary_cleaner(matches_rsk_fct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging all three arrays and storing it as csv for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1,df2,df3 = pd.DataFrame(array_one),pd.DataFrame(array_two),pd.DataFrame(array_three) #\n",
    "merged_dataframe = pd.concat([df1,df2,df3],axis = 1)\n",
    "merged_dataframe.columns = ['matches_mda','matches_qlty_qnt_disc','matches_rsk_fct']\n",
    "merged_dataframe.to_csv('merged_dataframe_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(r'D:\\nlp_internship\\submission\\merged_dataframe_all.csv') # loading the merged dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# concatenating the main data frame and newly merged data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataframe = pd.concat([df,df2],axis = 1)\n",
    "final_dataframe.drop(columns='Unnamed: 0',inplace = True)\n",
    "final_dataframe.fillna(' ',inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Index(['CIK', 'CONAME', 'FYRMO', 'FDATE', 'FORM', 'SECFNAME', 'matches_mda',\n       'matches_qlty_qnt_disc', 'matches_rsk_fct'],\n      dtype='object')"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "final_dataframe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_stopwords(columns):\n",
    "    \"\"\"This function is used to tokenize and removing everything except alphabets in the text files.\"\"\"\n",
    "    columns = columns.apply(lambda x : re.sub(\"[^a-zA-Z]\",\" \",str(x))) # removing everything except letters\n",
    "    columns = columns.apply(lambda x : wordpunct_tokenize(x)) # tokenizing the frame\n",
    "    # cleaning stopwords\n",
    "    columns = columns.apply(lambda x : [word.lower() for word in x if word not in stopword_list])\n",
    "    return columns\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a new data frame called the final data frame which consists of all the tokenized documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataframe['mda_bulk'] = cleaning_stopwords(final_dataframe['matches_mda'])\n",
    "final_dataframe['qltqnt_bulk'] = cleaning_stopwords(final_dataframe['matches_qlty_qnt_disc'])\n",
    "final_dataframe['rskfct_bulk'] = cleaning_stopwords(final_dataframe['matches_rsk_fct'])\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I have tried to compute the required variables using three methods this method is highly efficient and calculates all the variables in the number of minutes because I am creating a class called text analysis which will initialize the four variables words,complex_words,positive_words and negative words. hence for each of the three sections, I will create a new object for that section and compute all the 14 concerned variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class text_analysis:\n",
    "    def __init__(self,column_with_words,column_with_sentences):\n",
    "        self.column_with_words = column_with_words # initializing the column which contaions words.\n",
    "        self.column_with_sentences = column_with_sentences # initializing the words which contains sentence tokens.\n",
    "        self.words = self.column_with_words.apply(lambda x : len([word for word in x])) # initialing the variable wors count\n",
    "        self.complex_words = self.column_with_words.apply(lambda x : len([word for word in x if syllables.estimate(word) > 2])) # initializing the variable complex words\n",
    "        self.positive_words_column = self.column_with_words.apply(lambda x : len([word for word in x if word in np.array(positive_dict)])) # initializing the column number of positive words\n",
    "        self.negative_words_column = self.column_with_words.apply(lambda x : len([word for word in x if word in np.array(negative_dict)])) # initializing the column number of positive words\n",
    "    def positive_score(self):\n",
    "        '''This score is calculated by assigning the value of +1 for each word if \n",
    "        found in the Positive Dictionary and then adding up all the values. '''\n",
    "        return self.positive_words_column\n",
    "        #return self.column_with_words.apply(lambda x : len([word for word in x if word in positive_dict]))\n",
    "    def negative_score(self):\n",
    "        '''This score is calculated by assigning the value of +1 for each word if \n",
    "        found in the Negative Dictionary and then adding up all the values.'''\n",
    "        return self.negative_words_column\n",
    "        #return self.column_with_words.apply(lambda x : len([word for word in x if word in negative_dict]))\n",
    "    def polarity_score(self):\n",
    "        \"\"\"This is the score that determines if a given text is positive or negative in nature. It is calculated by using the formula: \n",
    "            Polarity Score = (Positive Score – Negative Score)/ ((Positive Score + Negative Score) + 0.000001)\n",
    "        \"\"\"\n",
    "        val1 = (self.positive_words_column - self.negative_words_column)\n",
    "        val2 = (self.positive_words_column + self.negative_words_column + 0.00001)\n",
    "        return val1 / val2\n",
    "    def average_sentence_length(self):\n",
    "        \"\"\"\n",
    "        Average Sentence Length = the number of words / the number of sentences\n",
    "        we use sent_tokenizer liabrary and we replace null values with 0\n",
    "\n",
    "        \"\"\"\n",
    "        #words = self.column_with_words.apply(lambda x : len([word for word in x]))\n",
    "        sentences = self.column_with_sentences.apply(lambda x : len(sent_tokenize(str(x))))\n",
    "        ans =  self.words / sentences\n",
    "        ans = ans.fillna(0)\n",
    "        return ans\n",
    "    def syllables_count(self,word):\n",
    "        \"\"\"This function is used to count the syllables. In this case, we use the cmu dictionary which consists \n",
    "        of all the syllables of given words but I found that there are some words which are not defined in the \n",
    "        cmu dictionary hence I used the algorithm to calculate the number of syllables.\"\"\"\n",
    "        phoneme_dictonary = dict(cmudict.entries())\n",
    "        count = 0\n",
    "        vowels = 'aeiouy'\n",
    "        word = word.lower()\n",
    "        if word[0] in vowels:\n",
    "            count += 1\n",
    "        for i in range(1,len(word)):\n",
    "            if word[i] in vowels and word[i - 1] not in vowels:\n",
    "                count += 1\n",
    "        if word[-1] == 'e':\n",
    "            count -= 1\n",
    "        if word[-2:] == 'le':\n",
    "            count += 1\n",
    "        if count == 0:\n",
    "            count += 1\n",
    "        # comparing the values with cmudict\n",
    "        try:\n",
    "            second_count = 0\n",
    "            for val in phoneme_dictonary[word.lower()]:\n",
    "                for val1 in val:\n",
    "                    if val1[-1].isdigit():\n",
    "                        second_count += len(val1)\n",
    "            return second_count\n",
    "        except KeyError: # this is in case the values are not found in the cmu dict\n",
    "            return count\n",
    "    def complex_words_proportion(self):\n",
    "        \"\"\"Percentage of Complex words = the number of complex words / the number of words \"\"\"\n",
    "        #complex_words = self.column_with_words.apply(lambda x : len([word for word in x if self.syllables_count(word) > 2]))\n",
    "        #words = self.column_with_words.apply(lambda x : len([word for word in x]))\n",
    "        ans = self.complex_words / self.words\n",
    "        ans = ans.fillna(0)\n",
    "        return ans\n",
    "\n",
    "    def fog_index(self):\n",
    "        \"\"\"Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)\n",
    "        The Gunning Fog Index gives the number of years of education that your reader \n",
    "        hypothetically needs to understand the paragraph or text. The Gunning Fog Index \n",
    "        formula implies that short sentences written in plain English achieve a better \n",
    "        score than long sentences written in complicated language.\"\"\"\n",
    "        return 0.4 * (self.average_sentence_length() + self.complex_words)\n",
    "\n",
    "    def complex_words_count(self):\n",
    "        \"\"\" Complex words are words in the text that contain more than two syllables.\n",
    "        hence we compare it with our words.\"\"\"\n",
    "        #complex_words = self.column_with_words.apply(lambda x : len([word for word in x if self.syllables_count(word) > 2]))\n",
    "        ans = self.complex_words.fillna(0) \n",
    "        \n",
    "        return ans\n",
    "    def word_counts(self):\n",
    "        \"\"\"This function is used to calculate number of words in each document axcept stopwords found in nltk library\"\"\"\n",
    "        stopwords_nltk = np.array(list(set(stopwords.words('english'))))\n",
    "        words = self.column_with_words.apply(lambda x : len([word for word in x if word not in stopwords_nltk]))\n",
    "        ans =  words\n",
    "        ans = ans.fillna(0)\n",
    "        return ans\n",
    "\n",
    "    def uncertainty_words_count(self):\n",
    "        uncertainty_dict = pd.read_excel(r'D:\\nlp_internship\\Data Science\\uncertainty_dictionary.xlsx')\n",
    "        uncertainty_dict = uncertainty_dict['Word'].str.lower().to_list()\n",
    "        words = self.column_with_words.apply(lambda x : len([word for word in x if word  in uncertainty_dict]))\n",
    "        return words\n",
    "    def constraning_words_count(self):\n",
    "        constrain_dict = pd.read_excel(r'D:\\nlp_internship\\Data Science\\constraining_dictionary.xlsx')\n",
    "        constrain_dict = constrain_dict['Word'].str.lower().to_list()\n",
    "        words = self.column_with_words.apply(lambda x : len([word for word in x if word  in constrain_dict]))\n",
    "        return words\n",
    "    def positive_words_proportion(self):\n",
    "        #words = self.column_with_words.apply(lambda x : len([word for word in x]))\n",
    "        ans = self.positive_words_column / self.words\n",
    "        ans = ans.fillna(0)\n",
    "        return ans\n",
    "    def negative_words_proportion(self):\n",
    "        #words = self.column_with_words.apply(lambda x : len([word for word in x]))\n",
    "        ans = self.negative_words_column / self.words\n",
    "        ans = ans.fillna(0)\n",
    "        return ans\n",
    "    def uncertaninty_words_proportion(self):\n",
    "        #words = self.column_with_words.apply(lambda x : len([word for word in x]))\n",
    "        ans = self.uncertainty_words_count() / self.words\n",
    "        ans = ans.fillna(0)\n",
    "        return ans\n",
    "    def constraining_words_proportion(self):\n",
    "        #words = self.column_with_words.apply(lambda x : len([word for word in x]))\n",
    "        ans = self.constraning_words_count() / self.words\n",
    "        ans = ans.fillna(0)\n",
    "        return ans\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Index(['CIK', 'CONAME', 'FYRMO', 'FDATE', 'FORM', 'SECFNAME', 'matches_mda',\n       'matches_qlty_qnt_disc', 'matches_rsk_fct', 'mda_bulk', 'qltqnt_bulk',\n       'rskfct_bulk'],\n      dtype='object')"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "final_dataframe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class text_analysis_vectorized:\n",
    "    def __init__(self,column_with_words,column_with_sentences):\n",
    "        self.column_with_words = column_with_words # initializing the column which contaions words.\n",
    "        self.column_with_sentences = column_with_sentences # initializing the words which contains sentence tokens.\n",
    "        self.words = self.column_with_words.apply(lambda x : len([word for word in x])) # initialing the variable wors count\n",
    "        self.complex_words = self.column_with_words.apply(lambda x : len([word for word in x if syllables.estimate(word) > 2])) # initializing the variable complex words\n",
    "        self.positive_words_column = self.column_with_words.apply(lambda x : len([word for word in x if word in np.array(positive_dict)])) # initializing the column number of positive words\n",
    "        self.negative_words_column = self.column_with_words.apply(lambda x : len([word for word in x if word in np.array(negative_dict)])) # initializing the column number of positive words\n",
    "    def positive_score(self):\n",
    "        '''This score is calculated by assigning the value of +1 for each word if \n",
    "        found in the Positive Dictionary and then adding up all the values. '''\n",
    "        return self.positive_words_column\n",
    "        #return self.column_with_words.apply(lambda x : len([word for word in x if word in positive_dict]))\n",
    "    def negative_score(self):\n",
    "        '''This score is calculated by assigning the value of +1 for each word if \n",
    "        found in the Negative Dictionary and then adding up all the values.'''\n",
    "        return self.negative_words_column\n",
    "        #return self.column_with_words.apply(lambda x : len([word for word in x if word in negative_dict]))\n",
    "    def polarity_score(self):\n",
    "        \"\"\"This is the score that determines if a given text is positive or negative in nature. It is calculated by using the formula: \n",
    "            Polarity Score = (Positive Score – Negative Score)/ ((Positive Score + Negative Score) + 0.000001)\n",
    "        \"\"\"\n",
    "        val1 = (self.positive_words_column - self.negative_words_column)\n",
    "        val2 = (self.positive_words_column + self.negative_words_column + 0.00001)\n",
    "        return val1 / val2\n",
    "    def average_sentence_length(self):\n",
    "        \"\"\"\n",
    "        Average Sentence Length = the number of words / the number of sentences\n",
    "        we use sent_tokenizer liabrary and we replace null values with 0\n",
    "\n",
    "        \"\"\"\n",
    "        #words = self.column_with_words.apply(lambda x : len([word for word in x]))\n",
    "        sentences = self.column_with_sentences.apply(lambda x : len(sent_tokenize(str(x))))\n",
    "        ans =  self.words / sentences\n",
    "        ans = ans.fillna(0)\n",
    "        return ans\n",
    "    def syllables_count(self,word):\n",
    "        \"\"\"This function is used to count the syllables. In this case, we use the cmu dictionary which consists \n",
    "        of all the syllables of given words but I found that there are some words which are not defined in the \n",
    "        cmu dictionary hence I used the algorithm to calculate the number of syllables.\"\"\"\n",
    "        phoneme_dictonary = dict(cmudict.entries())\n",
    "        count = 0\n",
    "        vowels = 'aeiouy'\n",
    "        word = word.lower()\n",
    "        if word[0] in vowels:\n",
    "            count += 1\n",
    "        for i in range(1,len(word)):\n",
    "            if word[i] in vowels and word[i - 1] not in vowels:\n",
    "                count += 1\n",
    "        if word[-1] == 'e':\n",
    "            count -= 1\n",
    "        if word[-2:] == 'le':\n",
    "            count += 1\n",
    "        if count == 0:\n",
    "            count += 1\n",
    "        # comparing the values with cmudict\n",
    "        try:\n",
    "            second_count = 0\n",
    "            for val in phoneme_dictonary[word.lower()]:\n",
    "                for val1 in val:\n",
    "                    if val1[-1].isdigit():\n",
    "                        second_count += len(val1)\n",
    "            return second_count\n",
    "        except KeyError: # this is in case the values are not found in the cmu dict\n",
    "            return count\n",
    "    def complex_words_proportion(self):\n",
    "        \"\"\"Percentage of Complex words = the number of complex words / the number of words \"\"\"\n",
    "        #complex_words = self.column_with_words.apply(lambda x : len([word for word in x if self.syllables_count(word) > 2]))\n",
    "        #words = self.column_with_words.apply(lambda x : len([word for word in x]))\n",
    "        ans = self.complex_words / self.words\n",
    "        ans = ans.fillna(0)\n",
    "        return ans\n",
    "\n",
    "    def fog_index(self):\n",
    "        \"\"\"Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)\n",
    "        The Gunning Fog Index gives the number of years of education that your reader \n",
    "        hypothetically needs to understand the paragraph or text. The Gunning Fog Index \n",
    "        formula implies that short sentences written in plain English achieve a better \n",
    "        score than long sentences written in complicated language.\"\"\"\n",
    "        return 0.4 * (self.average_sentence_length() + self.complex_words)\n",
    "\n",
    "    def complex_words_count(self):\n",
    "        \"\"\" Complex words are words in the text that contain more than two syllables.\n",
    "        hence we compare it with our words.\"\"\"\n",
    "        #complex_words = self.column_with_words.apply(lambda x : len([word for word in x if self.syllables_count(word) > 2]))\n",
    "        ans = self.complex_words.fillna(0) \n",
    "        \n",
    "        return ans\n",
    "    def word_counts(self):\n",
    "        \"\"\"This function is used to calculate number of words in each document axcept stopwords found in nltk library\"\"\"\n",
    "        stopwords_nltk = np.array(list(set(stopwords.words('english'))))\n",
    "        words = self.column_with_words.apply(lambda x : len([word for word in x if word not in stopwords_nltk]))\n",
    "        ans =  words\n",
    "        ans = ans.fillna(0)\n",
    "        return ans\n",
    "\n",
    "    def uncertainty_words_count(self):\n",
    "        uncertainty_dict = pd.read_excel(r'D:\\nlp_internship\\Data Science\\uncertainty_dictionary.xlsx')\n",
    "        uncertainty_dict = uncertainty_dict['Word'].str.lower().to_list()\n",
    "        words = self.column_with_words.apply(lambda x : len([word for word in x if word  in uncertainty_dict]))\n",
    "        return words\n",
    "    def constraning_words_count(self):\n",
    "        constrain_dict = pd.read_excel(r'D:\\nlp_internship\\Data Science\\constraining_dictionary.xlsx')\n",
    "        constrain_dict = constrain_dict['Word'].str.lower().to_list()\n",
    "        words = self.column_with_words.apply(lambda x : len([word for word in x if word  in constrain_dict]))\n",
    "        return words\n",
    "    def positive_words_proportion(self):\n",
    "        #words = self.column_with_words.apply(lambda x : len([word for word in x]))\n",
    "        ans = self.positive_words_column / self.words\n",
    "        ans = ans.fillna(0)\n",
    "        return ans\n",
    "    def negative_words_proportion(self):\n",
    "        #words = self.column_with_words.apply(lambda x : len([word for word in x]))\n",
    "        ans = self.negative_words_column / self.words\n",
    "        ans = ans.fillna(0)\n",
    "        return ans\n",
    "    def uncertaninty_words_proportion(self):\n",
    "        #words = self.column_with_words.apply(lambda x : len([word for word in x]))\n",
    "        ans = self.uncertainty_words_count() / self.words\n",
    "        ans = ans.fillna(0)\n",
    "        return ans\n",
    "    def constraining_words_proportion(self):\n",
    "        #words = self.column_with_words.apply(lambda x : len([word for word in x]))\n",
    "        ans = self.constraning_words_count() / self.words\n",
    "        ans = ans.fillna(0)\n",
    "        return ans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the mda object and giving it the concerned values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "mda = text_analysis(final_dataframe['mda_bulk'],final_dataframe['matches_mda']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "MDA = pd.DataFrame() #creating mda dataframe\n",
    "MDA['mda_positive_score'] = mda.positive_score()\n",
    "MDA['mda_negative_score'] = mda.negative_score()\n",
    "MDA['mda_polarity_score'] = mda.polarity_score()\n",
    "MDA['mda_average_sentence_length'] = mda.average_sentence_length()\n",
    "MDA['mda_percenmdage_of_complex_words'] = mda.complex_words_proportion()\n",
    "MDA['mda_fog_index'] = mda.fog_index()\n",
    "MDA['mda_complex_word_count'] = mda.complex_words_count()\n",
    "MDA['mda_word_count'] = mda.word_counts()\n",
    "MDA['mda_uncertainty_score'] = mda.uncertainty_words_count()\n",
    "MDA['mda_constraining_score'] = mda.constraning_words_count()\n",
    "MDA['mda_positive_word_proportion'] = mda.positive_words_proportion()\n",
    "MDA['mda_negative_word_proportion'] = mda.negative_words_proportion()\n",
    "MDA['mda_uncertainty_word_proportion'] = mda.uncertaninty_words_proportion()\n",
    "MDA['mda_constraining_word_proportion'] = mda.constraining_words_proportion()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "MDA.to_csv('mda.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "qqdmr = text_analysis(final_dataframe['qltqnt_bulk'],final_dataframe['matches_qlty_qnt_disc'])\n",
    "QQDMR = pd.DataFrame()\n",
    "QQDMR['qqdmr_positive_score'] = qqdmr.positive_score()\n",
    "QQDMR['qqdmr_negative_score'] = qqdmr.negative_score()\n",
    "QQDMR['qqdmr_polarity_score'] = qqdmr.polarity_score()\n",
    "QQDMR['qqdmr_average_sentence_length'] = qqdmr.average_sentence_length()\n",
    "QQDMR['qqdmr_percenqqdmrge_of_complex_words'] = qqdmr.complex_words_proportion()\n",
    "QQDMR['qqdmr_fog_index'] = qqdmr.fog_index()\n",
    "QQDMR['qqdmr_complex_word_count'] = qqdmr.complex_words_count()\n",
    "QQDMR['qqdmr_word_count'] = qqdmr.word_counts()\n",
    "QQDMR['qqdmr_uncertainty_score'] = qqdmr.uncertainty_words_count()\n",
    "QQDMR['qqdmr_constraining_score'] = qqdmr.constraning_words_count()\n",
    "QQDMR['qqdmr_positive_word_proportion'] = qqdmr.positive_words_proportion()\n",
    "QQDMR['qqdmr_negative_word_proportion'] = qqdmr.negative_words_proportion()\n",
    "QQDMR['qqdmr_uncertainty_word_proportion'] = qqdmr.uncertaninty_words_proportion()\n",
    "QQDMR['qqdmr_constraining_word_proportion'] = qqdmr.constraining_words_proportion()\n",
    "\n",
    "QQDMR.to_csv('qqdmr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = text_analysis(final_dataframe['rskfct_bulk'],final_dataframe['matches_rsk_fct'])\n",
    "RF = pd.DataFrame()\n",
    "RF['rf_positive_score'] = rf.positive_score()\n",
    "RF['rf_negative_score'] = rf.negative_score()\n",
    "RF['rf_polarity_score'] = rf.polarity_score()\n",
    "RF['rf_average_sentence_length'] = rf.average_sentence_length()\n",
    "RF['rf_percenrfge_of_complex_words'] = rf.complex_words_proportion()\n",
    "RF['rf_fog_index'] = rf.fog_index()\n",
    "RF['rf_complex_word_count'] = rf.complex_words_count()\n",
    "RF['rf_word_count'] = rf.word_counts()\n",
    "RF['rf_uncertainty_score'] = rf.uncertainty_words_count()\n",
    "RF['rf_constraining_score'] = rf.constraning_words_count()\n",
    "RF['rf_positive_word_proportion'] = rf.positive_words_proportion()\n",
    "RF['rf_negative_word_proportion'] = rf.negative_words_proportion()\n",
    "RF['rf_uncertainty_word_proportion'] = rf.uncertaninty_words_proportion()\n",
    "RF['rf_constraining_word_proportion'] = rf.constraining_words_proportion()\n",
    "RF.to_csv('rf.csv')\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([MDA,QQDMR,RF],axis = 1).to_csv('highly_efficient.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "     mda_positive_score  mda_negative_score  mda_polarity_score  \\\n0                    17                  62           -0.569620   \n1                     9                  46           -0.672727   \n2                     0                   0            0.000000   \n3                    39                 120           -0.509434   \n4                     0                   0            0.000000   \n..                  ...                 ...                 ...   \n147                   0                   0            0.000000   \n148                   0                   0            0.000000   \n149                   0                   0            0.000000   \n150                   0                   0            0.000000   \n151                   0                   0            0.000000   \n\n     mda_average_sentence_length  mda_percenmdage_of_complex_words  \\\n0                         2011.0                          0.521134   \n1                         1570.0                          0.505096   \n2                            0.0                          0.000000   \n3                         3812.0                          0.548006   \n4                            0.0                          0.000000   \n..                           ...                               ...   \n147                          0.0                          0.000000   \n148                          0.0                          0.000000   \n149                          0.0                          0.000000   \n150                          0.0                          0.000000   \n151                          0.0                          0.000000   \n\n     mda_fog_index  mda_complex_word_count  mda_word_count  \\\n0           1223.6                    1048            1915   \n1            945.2                     793            1504   \n2              0.0                       0               0   \n3           2360.4                    2089            3624   \n4              0.0                       0               0   \n..             ...                     ...             ...   \n147            0.0                       0               0   \n148            0.0                       0               0   \n149            0.0                       0               0   \n150            0.0                       0               0   \n151            0.0                       0               0   \n\n     mda_uncertainty_score  mda_constraining_score  \\\n0                       23                      10   \n1                       50                       2   \n2                        0                       0   \n3                       61                      39   \n4                        0                       0   \n..                     ...                     ...   \n147                      0                       0   \n148                      0                       0   \n149                      0                       0   \n150                      0                       0   \n151                      0                       0   \n\n     mda_positive_word_proportion  mda_negative_word_proportion  \\\n0                        0.008454                      0.030830   \n1                        0.005732                      0.029299   \n2                        0.000000                      0.000000   \n3                        0.010231                      0.031480   \n4                        0.000000                      0.000000   \n..                            ...                           ...   \n147                      0.000000                      0.000000   \n148                      0.000000                      0.000000   \n149                      0.000000                      0.000000   \n150                      0.000000                      0.000000   \n151                      0.000000                      0.000000   \n\n     mda_uncertainty_word_proportion  mda_constraining_word_proportion  \n0                           0.011437                          0.004973  \n1                           0.031847                          0.001274  \n2                           0.000000                          0.000000  \n3                           0.016002                          0.010231  \n4                           0.000000                          0.000000  \n..                               ...                               ...  \n147                         0.000000                          0.000000  \n148                         0.000000                          0.000000  \n149                         0.000000                          0.000000  \n150                         0.000000                          0.000000  \n151                         0.000000                          0.000000  \n\n[152 rows x 14 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mda_positive_score</th>\n      <th>mda_negative_score</th>\n      <th>mda_polarity_score</th>\n      <th>mda_average_sentence_length</th>\n      <th>mda_percenmdage_of_complex_words</th>\n      <th>mda_fog_index</th>\n      <th>mda_complex_word_count</th>\n      <th>mda_word_count</th>\n      <th>mda_uncertainty_score</th>\n      <th>mda_constraining_score</th>\n      <th>mda_positive_word_proportion</th>\n      <th>mda_negative_word_proportion</th>\n      <th>mda_uncertainty_word_proportion</th>\n      <th>mda_constraining_word_proportion</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>17</td>\n      <td>62</td>\n      <td>-0.569620</td>\n      <td>2011.0</td>\n      <td>0.521134</td>\n      <td>1223.6</td>\n      <td>1048</td>\n      <td>1915</td>\n      <td>23</td>\n      <td>10</td>\n      <td>0.008454</td>\n      <td>0.030830</td>\n      <td>0.011437</td>\n      <td>0.004973</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>9</td>\n      <td>46</td>\n      <td>-0.672727</td>\n      <td>1570.0</td>\n      <td>0.505096</td>\n      <td>945.2</td>\n      <td>793</td>\n      <td>1504</td>\n      <td>50</td>\n      <td>2</td>\n      <td>0.005732</td>\n      <td>0.029299</td>\n      <td>0.031847</td>\n      <td>0.001274</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>39</td>\n      <td>120</td>\n      <td>-0.509434</td>\n      <td>3812.0</td>\n      <td>0.548006</td>\n      <td>2360.4</td>\n      <td>2089</td>\n      <td>3624</td>\n      <td>61</td>\n      <td>39</td>\n      <td>0.010231</td>\n      <td>0.031480</td>\n      <td>0.016002</td>\n      <td>0.010231</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>150</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>151</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>152 rows × 14 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding constraining words for the whole report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n"
    }
   ],
   "source": [
    "complete_array = []\n",
    "count = 0\n",
    "for hyperlink in df['SECFNAME']:\n",
    "    text = urllib.request.urlopen(hyperlink).read().decode('utf-8')\n",
    "    count += 1\n",
    "    complete_array.append(preliminary_cleaner(text))\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_document_dataframe = pd.DataFrame()\n",
    "stopwords_nltk = np.array(list(set(stopwords.words('english'))))\n",
    "whole_document_dataframe['constraining'] = complete_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_document_dataframe = cleaning_stopwords(whole_document_dataframe['constraining'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = pd.DataFrame(whole_document_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_array = arr['constraining'].apply(lambda x : np.union1d(np.array(x),stopwords_nltk))\n",
    "uncertainty_dict = pd.read_excel(r'D:\\nlp_internship\\Data Science\\uncertainty_dictionary.xlsx')\n",
    "uncertainty_dict = np.array(uncertainty_dict['Word'].str.lower().to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = union_array.apply(lambda x : len(np.intersect1d(x,uncertainty_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}