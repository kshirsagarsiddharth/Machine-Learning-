{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing all the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import cmudict\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the ciklist into Pandas dataframe and doing some preliminary analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(r'D:\\nlp_internship\\Data Science\\cik_list.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(152, 6)"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "    CIK            CONAME   FYRMO      FDATE     FORM  \\\n0  3662  SUNBEAM CORP/FL/  199803 1998-03-06  10-K405   \n1  3662  SUNBEAM CORP/FL/  199805 1998-05-15     10-Q   \n2  3662  SUNBEAM CORP/FL/  199808 1998-08-13  NT 10-Q   \n3  3662  SUNBEAM CORP/FL/  199811 1998-11-12   10-K/A   \n4  3662  SUNBEAM CORP/FL/  199811 1998-11-16  NT 10-Q   \n\n                                   SECFNAME  \n0  edgar/data/3662/0000950170-98-000413.txt  \n1  edgar/data/3662/0000950170-98-001001.txt  \n2  edgar/data/3662/0000950172-98-000783.txt  \n3  edgar/data/3662/0000950170-98-002145.txt  \n4  edgar/data/3662/0000950172-98-001203.txt  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CIK</th>\n      <th>CONAME</th>\n      <th>FYRMO</th>\n      <th>FDATE</th>\n      <th>FORM</th>\n      <th>SECFNAME</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3662</td>\n      <td>SUNBEAM CORP/FL/</td>\n      <td>199803</td>\n      <td>1998-03-06</td>\n      <td>10-K405</td>\n      <td>edgar/data/3662/0000950170-98-000413.txt</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3662</td>\n      <td>SUNBEAM CORP/FL/</td>\n      <td>199805</td>\n      <td>1998-05-15</td>\n      <td>10-Q</td>\n      <td>edgar/data/3662/0000950170-98-001001.txt</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3662</td>\n      <td>SUNBEAM CORP/FL/</td>\n      <td>199808</td>\n      <td>1998-08-13</td>\n      <td>NT 10-Q</td>\n      <td>edgar/data/3662/0000950172-98-000783.txt</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3662</td>\n      <td>SUNBEAM CORP/FL/</td>\n      <td>199811</td>\n      <td>1998-11-12</td>\n      <td>10-K/A</td>\n      <td>edgar/data/3662/0000950170-98-002145.txt</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3662</td>\n      <td>SUNBEAM CORP/FL/</td>\n      <td>199811</td>\n      <td>1998-11-16</td>\n      <td>NT 10-Q</td>\n      <td>edgar/data/3662/0000950172-98-001203.txt</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                CIK          FYRMO\ncount    152.000000     152.000000\nmean    5861.605263  200220.750000\nstd     2310.633436     429.991117\nmin     3662.000000  199402.000000\n25%     4447.000000  199906.000000\n50%     5907.000000  200101.500000\n75%     6201.000000  200605.500000\nmax    12239.000000  201407.000000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CIK</th>\n      <th>FYRMO</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>152.000000</td>\n      <td>152.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>5861.605263</td>\n      <td>200220.750000</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>2310.633436</td>\n      <td>429.991117</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>3662.000000</td>\n      <td>199402.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>4447.000000</td>\n      <td>199906.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>5907.000000</td>\n      <td>200101.500000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>6201.000000</td>\n      <td>200605.500000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>12239.000000</td>\n      <td>201407.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## we need to extract the files from the web, hence hence lets add the hyperlink and extract all the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hyperlink = 'https://www.sec.gov/Archives/'\n",
    "df['SECFNAME'] = hyperlink + df['SECFNAME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'https://www.sec.gov/Archives/edgar/data/3662/0000950172-98-000783.txt'"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "df['SECFNAME'][2] # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find number of syllables in the word\n",
    "from nltk.corpus import cmudict\n",
    "phoneme_dictonary = dict(cmudict.entries())\n",
    "def syllables_count(word):\n",
    "    count = 0\n",
    "    vowels = 'aeiouy'\n",
    "    word = word.lower()\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "    for i in range(1,len(word)):\n",
    "        if word[i] in vowels and word[i - 1] not in vowels:\n",
    "            count += 1\n",
    "    if word[-1] == 'e':\n",
    "        count -= 1\n",
    "    if word[-2:] == 'le':\n",
    "        count += 1\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "    # comparing the values with cmudict\n",
    "    try:\n",
    "        second_count = 0\n",
    "        for val in phoneme_dictonary[word.lower()]:\n",
    "            for val1 in val:\n",
    "                if val1[-1].isdigit():\n",
    "                    second_count += len(val1)\n",
    "        return second_count\n",
    "    except KeyError: # this is in case the values are not found in the cmu dict\n",
    "        return count\n",
    "\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgt_disc_ana = r\"item[^a-zA-Z\\n]*\\d\\s*\\.\\s*management\\'s discussion and analysis.*?^\\s*item[^a-zA-Z\\n]*\\d\\s*\\.*\"\n",
    "qlty_qnt_disc = r\"item[^a-zA-Z\\n]*\\d[a-z]?\\.?\\s*Quantitative and Qualitative Disclosures about \" \\\n",
    "            r\"Market Risk.*?^\\s*item\\s*\\d\\s*\"\n",
    "rsk_fct = r\"item[^a-zA-Z\\n]*\\d[a-z]?\\.?\\s*Risk Factors.*?^\\s*item\\s*\\d\\s*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "constrain_dict = pd.read_excel(r'D:\\nlp_internship\\Data Science\\constraining_dictionary.xlsx')\n",
    "uncertainty_dict = pd.read_excel(r'D:\\nlp_internship\\Data Science\\uncertainty_dictionary.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataframe of positive and negative words\n",
    "negative_words = master_dict[master_dict['Negative'] == 2009]['Word']\n",
    "negative_words = negative_words.str.lower() # converting into lowercase\n",
    "# creating a dataframe of positive and negative words\n",
    "positive_words = master_dict[master_dict['Positive'] == 2009]['Word']\n",
    "positive_words = positive_words.str.lower() # converting into lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing with stopwords to remove the words\n",
    "negative_dict = []\n",
    "for val in stop_words[0].to_list():\n",
    "    for val1 in negative_words.to_list():\n",
    "        if val != val1:\n",
    "            negative_dict.append(val1)\n",
    "# comparing with stopwords to remove the words\n",
    "positive_dict = []\n",
    "for val in stop_words[0].to_list():\n",
    "    for val1 in positive_words.to_list():\n",
    "        if val != val1:\n",
    "            positive_dict.append(val1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preliminary_cleaner(text):\n",
    "    if text:\n",
    "        array = []\n",
    "        soup =text\n",
    "        soup = re.sub('[\\d%/$]','',str(soup))\n",
    "        soup = re.sub(\"\\\\\\\\n\",'',soup)\n",
    "        soup = ' '.join(soup.split())\n",
    "        return soup\n",
    "        \n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "' if matches_rsk_fct:\\n    soup = matches_rsk_fct\\n    soup = re.sub(\\'[\\\\d%/$]\\',\\'\\',str(soup))\\n    soup = re.sub(\"\\\\\\\\n\",\\'\\',soup)\\n    soup = \\' \\'.join(set(soup.split()))\\n    array_three.append(soup)\\nelse:\\n    array_three.append(None)'"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "\n",
    "array_one,array_two,array_three = [],[],[]\n",
    "for hyperlink in df['SECFNAME']:\n",
    "    text = urllib.request.urlopen(hyperlink).read().decode('utf-8')\n",
    "    matches_mda = re.findall(mgt_disc_ana, text, re.IGNORECASE | re.DOTALL | re.MULTILINE)\n",
    "    matches_qlty_qnt_disc= re.findall(qlty_qnt_disc,text,re.IGNORECASE | re.DOTALL | re.MULTILINE)\n",
    "    matches_rsk_fct = re.findall(rsk_fct,text,re.IGNORECASE | re.DOTALL | re.MULTILINE) \n",
    "    array_one.append(preliminary_cleaner(matches_mda))\n",
    "    array_two.append(preliminary_cleaner(matches_qlty_qnt_disc))\n",
    "    array_three.append(preliminary_cleaner(matches_rsk_fct))\n",
    "''' if matches_mda:\n",
    "    soup = matches_mda\n",
    "    soup = re.sub('[\\d%/$]','',str(soup))\n",
    "    soup = re.sub(\"\\\\\\\\n\",'',soup)\n",
    "    soup = ' '.join(set(soup.split()))\n",
    "    array_one.append(soup)\n",
    "\n",
    "else:\n",
    "    array_one.append(None)'''\n",
    "    \n",
    "''' if matches_qlty_qnt_disc:\n",
    "    soup = matches_qlty_qnt_disc\n",
    "    soup = re.sub('[\\d%/$]','',str(soup))\n",
    "    soup = re.sub(\"\\\\\\\\n\",'',soup)\n",
    "    soup = ' '.join(set(soup.split()))\n",
    "    array_two.append(soup)\n",
    "\n",
    "else:\n",
    "    array_two.append(None)'''\n",
    "\n",
    "''' if matches_rsk_fct:\n",
    "    soup = matches_rsk_fct\n",
    "    soup = re.sub('[\\d%/$]','',str(soup))\n",
    "    soup = re.sub(\"\\\\\\\\n\",'',soup)\n",
    "    soup = ' '.join(set(soup.split()))\n",
    "    array_three.append(soup)\n",
    "else:\n",
    "    array_three.append(None)'''\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "      \n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1,df2,df3 = pd.DataFrame(array_one),pd.DataFrame(array_two),pd.DataFrame(array_three)\n",
    "merged_dataframe = pd.concat([df1,df2,df3],axis = 1)\n",
    "merged_dataframe.columns = ['matches_mda','matches_qlty_qnt_disc','matches_rsk_fct']\n",
    "merged_dataframe.to_csv('merged_dataframe_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "       CIK            CONAME   FYRMO      FDATE     FORM  \\\n0     3662  SUNBEAM CORP/FL/  199803 1998-03-06  10-K405   \n1     3662  SUNBEAM CORP/FL/  199805 1998-05-15     10-Q   \n2     3662  SUNBEAM CORP/FL/  199808 1998-08-13  NT 10-Q   \n3     3662  SUNBEAM CORP/FL/  199811 1998-11-12   10-K/A   \n4     3662  SUNBEAM CORP/FL/  199811 1998-11-16  NT 10-Q   \n..     ...               ...     ...        ...      ...   \n147  12239       SPHERIX INC  200704 2007-04-02     10-K   \n148  12239       SPHERIX INC  200705 2007-05-16  NT 10-Q   \n149  12239       SPHERIX INC  200705 2007-05-18     10-Q   \n150  12239       SPHERIX INC  200705 2007-05-23   10-K/A   \n151  12239       SPHERIX INC  200708 2007-08-14     10-Q   \n\n                                              SECFNAME  \n0    https://www.sec.gov/Archives/edgar/data/3662/0...  \n1    https://www.sec.gov/Archives/edgar/data/3662/0...  \n2    https://www.sec.gov/Archives/edgar/data/3662/0...  \n3    https://www.sec.gov/Archives/edgar/data/3662/0...  \n4    https://www.sec.gov/Archives/edgar/data/3662/0...  \n..                                                 ...  \n147  https://www.sec.gov/Archives/edgar/data/12239/...  \n148  https://www.sec.gov/Archives/edgar/data/12239/...  \n149  https://www.sec.gov/Archives/edgar/data/12239/...  \n150  https://www.sec.gov/Archives/edgar/data/12239/...  \n151  https://www.sec.gov/Archives/edgar/data/12239/...  \n\n[152 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CIK</th>\n      <th>CONAME</th>\n      <th>FYRMO</th>\n      <th>FDATE</th>\n      <th>FORM</th>\n      <th>SECFNAME</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3662</td>\n      <td>SUNBEAM CORP/FL/</td>\n      <td>199803</td>\n      <td>1998-03-06</td>\n      <td>10-K405</td>\n      <td>https://www.sec.gov/Archives/edgar/data/3662/0...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3662</td>\n      <td>SUNBEAM CORP/FL/</td>\n      <td>199805</td>\n      <td>1998-05-15</td>\n      <td>10-Q</td>\n      <td>https://www.sec.gov/Archives/edgar/data/3662/0...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3662</td>\n      <td>SUNBEAM CORP/FL/</td>\n      <td>199808</td>\n      <td>1998-08-13</td>\n      <td>NT 10-Q</td>\n      <td>https://www.sec.gov/Archives/edgar/data/3662/0...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3662</td>\n      <td>SUNBEAM CORP/FL/</td>\n      <td>199811</td>\n      <td>1998-11-12</td>\n      <td>10-K/A</td>\n      <td>https://www.sec.gov/Archives/edgar/data/3662/0...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3662</td>\n      <td>SUNBEAM CORP/FL/</td>\n      <td>199811</td>\n      <td>1998-11-16</td>\n      <td>NT 10-Q</td>\n      <td>https://www.sec.gov/Archives/edgar/data/3662/0...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>12239</td>\n      <td>SPHERIX INC</td>\n      <td>200704</td>\n      <td>2007-04-02</td>\n      <td>10-K</td>\n      <td>https://www.sec.gov/Archives/edgar/data/12239/...</td>\n    </tr>\n    <tr>\n      <th>148</th>\n      <td>12239</td>\n      <td>SPHERIX INC</td>\n      <td>200705</td>\n      <td>2007-05-16</td>\n      <td>NT 10-Q</td>\n      <td>https://www.sec.gov/Archives/edgar/data/12239/...</td>\n    </tr>\n    <tr>\n      <th>149</th>\n      <td>12239</td>\n      <td>SPHERIX INC</td>\n      <td>200705</td>\n      <td>2007-05-18</td>\n      <td>10-Q</td>\n      <td>https://www.sec.gov/Archives/edgar/data/12239/...</td>\n    </tr>\n    <tr>\n      <th>150</th>\n      <td>12239</td>\n      <td>SPHERIX INC</td>\n      <td>200705</td>\n      <td>2007-05-23</td>\n      <td>10-K/A</td>\n      <td>https://www.sec.gov/Archives/edgar/data/12239/...</td>\n    </tr>\n    <tr>\n      <th>151</th>\n      <td>12239</td>\n      <td>SPHERIX INC</td>\n      <td>200708</td>\n      <td>2007-08-14</td>\n      <td>10-Q</td>\n      <td>https://www.sec.gov/Archives/edgar/data/12239/...</td>\n    </tr>\n  </tbody>\n</table>\n<p>152 rows Ã— 6 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(r'D:\\nlp_internship\\submission\\merged_dataframe_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataframe = pd.concat([df,df2],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataframe.drop(columns='Unnamed: 0',inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Sentiment Analysis\n",
    "    # 1.1 Removing all stop words\n",
    "stop_words = pd.read_csv(r'D:\\nlp_internship\\Data Science\\stop_words\\StopWords_GenericLong.txt',header=None)\n",
    "stopword_list = stop_words[0].to_list()\n",
    "master_dict = pd.read_csv(r'D:\\nlp_internship\\Data Science\\stop_words\\LoughranMcDonald_MasterDictionary_2018.csv')\n",
    "# creating dictonary of positive and negative words\n",
    "# creating a dataframe of positive and negative words\n",
    "negative_words = master_dict[master_dict['Negative'] == 2009]['Word']\n",
    "negative_words = negative_words.str.lower() # converting into lowercase\n",
    "# creating a dataframe of positive and negative words\n",
    "positive_words = master_dict[master_dict['Positive'] == 2009]['Word']\n",
    "positive_words = positive_words.str.lower() # converting into lowercase\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "final_dataframe.fillna(' ',inplace = True)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#letters_only = re.sub(\"[^a-zA-Z]\",\" \",lower_case) \n",
    "final_dataframe['matches_mda'] = final_dataframe['matches_mda'].apply(lambda x : re.sub(\"[^a-zA-Z]\",\" \",x)) # removing everything except letters\n",
    "final_dataframe['matches_mda'] = final_dataframe['matches_mda'].apply(lambda x : wordpunct_tokenize(x)) # tokenizing the frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning stopwords\n",
    "final_dataframe['matches_mda'] = final_dataframe['matches_mda'].apply(lambda x : [word.lower() for word in x if word not in stopword_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0    62\n1    46\n2     0\nName: matches_mda, dtype: int64"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "final_dataframe['matches_mda'].apply(lambda x : len([word for word in x if word in negative_dict]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "17"
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "len([word for word in final_dataframe.matches_mda[0] if word in positive_dict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positive_score(variable,data_frame):\n",
    "    return data_frame[variable].apply(lambda x : len([word for word in x if word in positive_dict]))\n",
    "def negative_score(variable,data_frame):\n",
    "    return data_frame[variable].apply(lambda x : len([word for word in x if word in negative_dict]))\n",
    "def polarity_score(positive_words_dataframe,negative_words_dataframe):\n",
    "    val1 = (positive_words_dataframe - negative_words_dataframe)\n",
    "    val2 = (positive_words_dataframe + negative_words_dataframe + 0.00001)\n",
    "    return val1 / val2\n",
    "def average_sentence_length(column_with_words,column_with_sentences):\n",
    "    words = column_with_words.apply(lambda x : len([word for word in x]))\n",
    "    sentences = column_with_sentences.apply(lambda x : len(sent_tokenize(str(x))))\n",
    "    return words / sentences\n",
    "phoneme_dictonary = dict(cmudict.entries())\n",
    "def syllables_count(word):\n",
    "    count = 0\n",
    "    vowels = 'aeiouy'\n",
    "    word = word.lower()\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "    for i in range(1,len(word)):\n",
    "        if word[i] in vowels and word[i - 1] not in vowels:\n",
    "            count += 1\n",
    "    if word[-1] == 'e':\n",
    "        count -= 1\n",
    "    if word[-2:] == 'le':\n",
    "        count += 1\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "    # comparing the values with cmudict\n",
    "    try:\n",
    "        second_count = 0\n",
    "        for val in phoneme_dictonary[word.lower()]:\n",
    "            for val1 in val:\n",
    "                if val1[-1].isdigit():\n",
    "                    second_count += len(val1)\n",
    "        return second_count\n",
    "    except KeyError: # this is in case the values are not found in the cmu dict\n",
    "        return count\n",
    "def complex_words(column_with_words):\n",
    "    complex_words = column_with_words.apply(lambda x : len([word for word in x if syllables_count(word) > 2]))\n",
    "    words = column_with_words.apply(lambda x : len([word for word in x]))\n",
    "    ans = complex_words / words\n",
    "    ans = ans.fillna(0)\n",
    "    return ans\n",
    "\n",
    "def fog_index(column_with_words,column_with_sentences):\n",
    "    return 0.4 * (average_sentence_length(column_with_words,column_with_sentences) + complex_words(column_with_words))\n",
    "\n",
    "def complex_words_count(column_with_words):\n",
    "    complex_words = column_with_words.apply(lambda x : len([word for word in x if syllables_count(word) > 2]))\n",
    "    ans = complex_words \n",
    "    ans = ans.fillna(0)\n",
    "    return ans\n",
    "def word_counts(column_with_words):\n",
    "    stopwords_nltk = np.array(list(set(stopwords.words('english'))))\n",
    "    words = column_with_words.apply(lambda x : len([word for word in x if word not in stopwords_nltk]))\n",
    "    ans =  words\n",
    "    ans = ans.fillna(0)\n",
    "    return ans\n",
    "\n",
    "def uncertainty_words_count(column_with_words):\n",
    "    uncertainty_dict = pd.read_excel(r'D:\\nlp_internship\\Data Science\\uncertainty_dictionary.xlsx')\n",
    "    uncertainty_dict = uncertainty_dict['Word'].str.lower().to_list()\n",
    "    words = column_with_words.apply(lambda x : len([word for word in x if word  in uncertainty_dict]))\n",
    "    return words\n",
    "def constraning_words_count(column_with_words):\n",
    "    constrain_dict = pd.read_excel(r'D:\\nlp_internship\\Data Science\\constraining_dictionary.xlsx')\n",
    "    constrain_dict = constrain_dict['Word'].str.lower().to_list()\n",
    "    words = column_with_words.apply(lambda x : len([word for word in x if word  in constrain_dict]))\n",
    "    return words\n",
    "def positive_words_proportion(positive_words_column,column_with_words):\n",
    "    words = column_with_words.apply(lambda x : len([word for word in x]))\n",
    "    ans = positive_words_column / words\n",
    "    ans = ans.fillna(0)\n",
    "    return ans\n",
    "def negative_words_proportion(negative_words_column,column_with_words):\n",
    "    words = column_with_words.apply(lambda x : len([word for word in x]))\n",
    "    ans = negative_words_column / words\n",
    "    ans = ans.fillna(0)\n",
    "    return ans\n",
    "def uncertaninty_words_proportion(column_with_words):\n",
    "    words = column_with_words.apply(lambda x : len([word for word in x]))\n",
    "    ans = uncertainty_words_count(column_with_words) / words\n",
    "    ans = ans.fillna(0)\n",
    "    return ans\n",
    "def constraining_words_proportion(column_with_words):\n",
    "    words = column_with_words.apply(lambda x : len([word for word in x]))\n",
    "    ans = constraning_words_count(column_with_words) / words\n",
    "    ans = ans.fillna(0)\n",
    "    return ans\n",
    "\n",
    "\n",
    "\n",
    "\n",
    ""
   ]
  }
 ]
}