{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scikitplot.metrics import plot_confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from time import time\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "",
    "_uuid": ""
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"../input/clean-16-million-tweets/clean_tweets_again.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace = True)\n",
    "df.drop(columns = 'Unnamed: 0',inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text']\n",
    "y = df['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# here data is split into three chunks namely train set,developement set and test set\n",
    "#### Train set:sample data that is used for learning\n",
    "### Developement set:the data used to tune the hyperparameters of the algorithm.\n",
    "### test set:the data used to test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data is split in 98|1|1\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_validation_test,y_train,y_validation_test = train_test_split(X,y,test_size = 0.02,random_state = 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validation,X_test,y_validation,y_test = train_test_split(X_validation_test,y_validation_test,test_size = 0.5,random_state = 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of training set is 1564120 with positive entries as 49.99469350177736 and negative entries as 50.005306498222645 \n",
      "\n",
      "The shape of validation set is 15960 with positive entries as 49.85588972431078 and negative entries as 50.14411027568922 \n",
      "\n",
      "The shape of test set is 15961 with positive entries as 49.55829835223357 and negative entries as 50.441701647766436\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.DataFrame(y_train)\n",
    "percent_positive = len(train_df[train_df['sentiment'] == 1])/len(train_df)\n",
    "percent_negative = len(train_df[train_df['sentiment'] == 0])/len(train_df)\n",
    "print('The shape of training set is',len(X_train),'with positive entries as',percent_positive*100,'and negative entries as',percent_negative*100,'\\n')\n",
    "validation_df = pd.DataFrame(y_validation)\n",
    "percent_positive = len(validation_df[validation_df['sentiment'] == 1])/len(validation_df)\n",
    "percent_negative = len(validation_df[validation_df['sentiment'] == 0])/len(validation_df)\n",
    "print('The shape of validation set is',len(X_validation),'with positive entries as',percent_positive*100,'and negative entries as',percent_negative *100,'\\n')\n",
    "test_df = pd.DataFrame(y_test)\n",
    "percent_positive = len(test_df[test_df['sentiment'] == 1])/len(test_df)\n",
    "percent_negative = len(test_df[test_df['sentiment'] == 0])/len(test_df)\n",
    "print('The shape of test set is',len(X_test),'with positive entries as',percent_positive*100,'and negative entries as',percent_negative*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**during comparision of various machine learning algorithms baseline is used as point of refrence to compare for this we will be using the zero classifier which only identifies majority class even though there is predicting power in this classifier this will be useful in determining the baseline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**another baseline for the classification of data is text blob which has a builtin sentiment classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=-1, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=8,\n",
      "                   warm_start=False)\n",
      "\n",
      "\n",
      "validation result for 10000 features\n",
      "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=  40.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   37.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=  37.2s\n",
      "the training and testing time is  77.71299409866333  seconds\n",
      "accurcy 0.8008270158511371  greatet than null accuracy  0.5044170164776643\n",
      "validation result for 20000 features\n",
      "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=  39.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   38.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=  38.9s\n",
      "the training and testing time is  78.23821806907654  seconds\n",
      "accurcy 0.8037090407869181  greatet than null accuracy  0.5044170164776643\n",
      "validation result for 30000 features\n",
      "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=  38.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   41.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=  42.0s\n",
      "the training and testing time is  81.1456995010376  seconds\n",
      "accurcy 0.804460873378861  greatet than null accuracy  0.5044170164776643\n",
      "validation result for 40000 features\n",
      "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=  38.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   45.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=  46.0s\n",
      "the training and testing time is  85.07326579093933  seconds\n",
      "accurcy 0.8052753586867991  greatet than null accuracy  0.5044170164776643\n",
      "validation result for 50000 features\n",
      "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=  38.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   47.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=  47.5s\n",
      "the training and testing time is  86.73376631736755  seconds\n",
      "accurcy 0.8042729152308753  greatet than null accuracy  0.5044170164776643\n",
      "validation result for 60000 features\n",
      "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=  38.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   50.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=  50.6s\n",
      "the training and testing time is  89.7258951663971  seconds\n",
      "accurcy 0.8040849570828895  greatet than null accuracy  0.5044170164776643\n",
      "validation result for 70000 features\n",
      "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=  39.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   54.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=  55.0s\n",
      "the training and testing time is  94.39271235466003  seconds\n",
      "accurcy 0.8048994423908277  greatet than null accuracy  0.5044170164776643\n",
      "validation result for 80000 features\n",
      "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=  39.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   57.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=  57.3s\n",
      "the training and testing time is  97.4627673625946  seconds\n",
      "accurcy 0.80552596955078  greatet than null accuracy  0.5044170164776643\n",
      "validation result for 90000 features\n",
      "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=  38.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   59.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=  59.7s\n",
      "the training and testing time is  99.04488325119019  seconds\n",
      "accurcy 0.8052753586867991  greatet than null accuracy  0.5044170164776643\n",
      "validation result for 100000 features\n",
      "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=  38.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total= 1.1min\n",
      "the training and testing time is  104.06822800636292  seconds\n",
      "accurcy 0.8062151494267277  greatet than null accuracy  0.5044170164776643\n"
     ]
    }
   ],
   "source": [
    "if len(X_test[y_test == 0])/len(X_test) > 0.5:\n",
    "    null_accuracy = len(X_test[y_test == 0])/len(X_test)\n",
    "else:\n",
    "    null_accuracy = 1 - len(X_test[y_test == 0])/len(X_test)\n",
    "\n",
    "number_of_features = np.arange(10000,100001,10000)\n",
    "clf = LogisticRegression(n_jobs = -1,verbose = 8)\n",
    "count_vec = TfidfVectorizer()\n",
    "result = []\n",
    "print(clf)\n",
    "print('\\n')\n",
    "for number in number_of_features:\n",
    "    count_vec.set_params(stop_words = None,max_features = number,ngram_range = (1,1))\n",
    "    pipeline = Pipeline([('vectorizer',count_vec),('classifier',clf)],verbose = 1)\n",
    "    print('validation result for {} features'.format(number))\n",
    "    t0 = time()\n",
    "    pipeline.fit(X_train,y_train)\n",
    "    pred = pipeline.predict(X_test)\n",
    "    train_test_time = time() - t0\n",
    "    accuracy = accuracy_score(pred,y_test)\n",
    "    print('the training and testing time is ',train_test_time,' seconds')\n",
    "    if accuracy > null_accuracy:\n",
    "        print('accurcy',accuracy,' greatet than null accuracy ',null_accuracy)\n",
    "    else:\n",
    "        print('accurcy',accuracy,' less than null accuracy ',null_accuracy)\n",
    "result.append((number,accuracy,train_test_time))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computation with unigram and without stopwords\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=-1, penalty='l2',\n",
      "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=8,\n",
      "                   warm_start=False)\n",
      "\n",
      "\n",
      "validation result for 10000 features\n",
      "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=  35.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   33.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=  33.1s\n",
      "the training and testing time is  68.61607813835144  seconds\n",
      "accuracy 0.7728212518012656 greater then null accuracy 0.5044170164776643\n",
      "\n",
      "\n",
      "validation result for 20000 features\n",
      "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=  34.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   35.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=  35.8s\n",
      "the training and testing time is  70.95119047164917  seconds\n",
      "accuracy 0.7741369588371656 greater then null accuracy 0.5044170164776643\n",
      "\n",
      "\n",
      "validation result for 30000 features\n",
      "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=  35.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   35.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=  35.2s\n",
      "the training and testing time is  70.75848126411438  seconds\n",
      "accuracy 0.7753900131570703 greater then null accuracy 0.5044170164776643\n",
      "\n",
      "\n",
      "validation result for 40000 features\n",
      "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=  34.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   40.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=  40.4s\n",
      "the training and testing time is  75.48648738861084  seconds\n",
      "accuracy 0.7757032767370465 greater then null accuracy 0.5044170164776643\n",
      "\n",
      "\n",
      "validation result for 50000 features\n",
      "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=  35.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   43.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=  43.3s\n",
      "the training and testing time is  78.73252630233765  seconds\n",
      "accuracy 0.7769563310569513 greater then null accuracy 0.5044170164776643\n",
      "\n",
      "\n",
      "validation result for 60000 features\n",
      "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=  35.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   45.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=  45.1s\n",
      "the training and testing time is  80.72434663772583  seconds\n",
      "accuracy 0.776079193033018 greater then null accuracy 0.5044170164776643\n",
      "\n",
      "\n",
      "validation result for 70000 features\n",
      "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=  35.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   46.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=  46.4s\n",
      "the training and testing time is  81.85373854637146  seconds\n",
      "accuracy 0.7765177620449847 greater then null accuracy 0.5044170164776643\n",
      "\n",
      "\n",
      "validation result for 80000 features\n",
      "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=  35.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   50.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=  50.7s\n",
      "the training and testing time is  86.20836019515991  seconds\n",
      "accuracy 0.7773322473529227 greater then null accuracy 0.5044170164776643\n",
      "\n",
      "\n",
      "validation result for 90000 features\n",
      "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=  35.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   54.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=  54.6s\n",
      "the training and testing time is  90.18562126159668  seconds\n",
      "accuracy 0.7763924566129942 greater then null accuracy 0.5044170164776643\n",
      "\n",
      "\n",
      "validation result for 100000 features\n",
      "[Pipeline] ........ (step 1 of 2) Processing vectorizer, total=  35.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:   56.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=  56.7s\n",
      "the training and testing time is  92.42504501342773  seconds\n",
      "accuracy 0.7770189837729465 greater then null accuracy 0.5044170164776643\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('computation with unigram and without stopwords')\n",
    "if len(X_test[y_test == 0])/len(X_test) > 0.5:\n",
    "    null_accuracy = len(X_test[y_test == 0])/len(X_test)\n",
    "else:\n",
    "    null_accuracy = 1 - len(x_test[y_test == 0])/len(X_test)\n",
    "number_of_features = np.arange(10000,100001,10000)\n",
    "clf = LogisticRegression(n_jobs = -1,verbose = 8)\n",
    "count_vec = TfidfVectorizer(stop_words = 'english')\n",
    "result_unigram_without_stopwords = []\n",
    "print(clf)\n",
    "print('\\n')\n",
    "for number in number_of_features:\n",
    "    count_vec.set_params(stop_words = 'english',max_features = number,ngram_range = (1,1))\n",
    "    pipeline = Pipeline([('vectorizer',count_vec),('classifier',clf)],verbose = 1)\n",
    "    print('validation result for {} features'.format(number))\n",
    "    t0 = time()\n",
    "    pipeline.fit(X_train,y_train)\n",
    "    pred = pipeline.predict(X_test)\n",
    "    train_test_time = time() - t0\n",
    "    accuracy = accuracy_score(pred,y_test)\n",
    "    print('the training and testing time is ',train_test_time,' seconds')\n",
    "    if accuracy > null_accuracy:\n",
    "        print('accuracy',accuracy,'greater then null accuracy',null_accuracy)\n",
    "    else:\n",
    "        print('accuracy',accuracy,'less than null accuracy',null_accuracy)\n",
    "    print('\\n')  \n",
    "result_unigram_without_stopwords.append((number,accuracy,train_test_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**lets create a list of custom stopwords in this case we will add custom stopwords selected from top 10 features in the list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "",
    "_uuid": ""
   },
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(result_unigram_without_stopwords)\n",
    "df1.to_csv('df2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(result)\n",
    "df2.to_csv('df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
